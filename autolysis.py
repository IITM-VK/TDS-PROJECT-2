# -*- coding: utf-8 -*-
"""autolysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17uDh-1DOxKVFDk3BwTRDkBa1h_xL2EZh
"""

import os
import sys
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from openai import ChatCompletion
import openai
from dotenv import load_dotenv

# Load the .env file
load_dotenv()

# Ensure the environment variable for AI Proxy token is set
AIPROXY_TOKEN = os.getenv("AIPROXY_TOKEN")
if not AIPROXY_TOKEN:
    print("Error: AIPROXY_TOKEN environment variable not set.")
    sys.exit(1)

# Initialize OpenAI client
openai.api_key = AIPROXY_TOKEN

def detect_outliers(df):
    """Detect outliers using the IQR method for numeric columns."""
    outlier_info = {}
    numeric_columns = df.select_dtypes(include=['number']).columns

    for column in numeric_columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
        outlier_info[column] = {
            "lower_bound": lower_bound,
            "upper_bound": upper_bound,
            "num_outliers": len(outliers)
        }

    return outlier_info

def analyze_dataset(file_path):
    # Load the dataset
    try:
        df = pd.read_csv(file_path)
    except Exception as e:
        print(f"Error loading file: {e}")
        sys.exit(1)

    # Perform generic analysis
    analysis = {
        "columns": list(df.columns),
        "dtypes": df.dtypes.apply(str).to_dict(),
        "summary_stats": df.describe(include='number').to_dict(),
        "missing_values": df.isnull().sum().to_dict()
    }

    # Detect outliers
    analysis["outliers"] = detect_outliers(df)

    return df, analysis

def generate_visualizations(df, output_dir):
    # Generate a correlation heatmap if applicable
    numeric_columns = df.select_dtypes(include=['number']).columns
    if len(numeric_columns) > 1:
        corr = df[numeric_columns].corr()
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr, annot=True, cmap="coolwarm")
        plt.title("Correlation Heatmap")
        plt.savefig(os.path.join(output_dir, "correlation_heatmap.png"))
        plt.close()

def visualize_distributions(data, output_dir):
        """Generate and save feature distribution histograms."""
        numeric_data = data.select_dtypes(include=[np.number])
        num_columns = len(numeric_data.columns)
        num_rows = (num_columns // 3) + (1 if num_columns % 3 != 0 else 0)
        plt.figure(figsize=(12, 4 * num_rows))
        numeric_data.hist(bins=30, figsize=(12, 8))
        plt.tight_layout()
        histogram_path = os.path.join(output_dir, "feature_distributions.png")
        plt.savefig(histogram_path, dpi=100)
        plt.close()


def narrate_story(analysis, output_dir):
    # Generate a narrative using LLM
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a data scientist narrating the story of a dataset."},
                {"role": "user", "content": f"Here's the analysis of the dataset: {analysis}"}
            ]
        )
        story = response['choices'][0]['message']['content']
    except Exception as e:
        story = f"Error generating narrative: {e}"

    # Write the story to README.md
    with open(os.path.join(output_dir, "README.md"), "w") as f:
        f.write(story)

def analyze_and_generate_output(file_path):
    # Define output directory based on file name
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    output_dir = os.path.join(".", base_name)
    os.makedirs(output_dir, exist_ok=True)

    # Analyze dataset
    df, analysis = analyze_dataset(file_path)

    # Generate visualizations
    generate_visualizations(df, output_dir)
    visualize_distributions(df, output_dir)

    # Narrate the story
    narrate_story(analysis, output_dir)

    # Save analysis to a JSON file
    analysis_path = os.path.join(output_dir, "analysis.json")
    pd.DataFrame([analysis]).to_json(analysis_path, orient='records', indent=4)

    return output_dir

def main():
    if len(sys.argv) < 2:
        print("Usage: python autolysis.py dataset1.csv dataset2.csv ...")
        sys.exit(1)

    file_paths = sys.argv[1:]
    output_dirs = []

    # Process each dataset file
    for file_path in file_paths:
        if os.path.exists(file_path):
            output_dir = analyze_and_generate_output(file_path)
            output_dirs.append(output_dir)
        else:
            print(f"File {file_path} not found!")

    print(f"Analysis completed. Results saved in directories: {', '.join(output_dirs)}")

if __name__ == "__main__":
    main()