# -*- coding: utf-8 -*-
"""autolysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17uDh-1DOxKVFDk3BwTRDkBa1h_xL2EZh
"""

!pip install uvicorn

import os
os.environ["AIPROXY_TOKEN"] = "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"

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import openai
from tenacity import retry, stop_after_attempt, wait_fixed

# Load API Key securely
AIPROXY_TOKEN = os.getenv("AIPROXY_TOKEN", "")  # Use environment variable for better security
if not AIPROXY_TOKEN:
    print("Error: AIPROXY_TOKEN environment variable is not set.")
    sys.exit(1)

openai.api_key = AIPROXY_TOKEN

@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def call_gpt(prompt, model="gpt-4", max_tokens=500):
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a data analysis assistant."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
            temperature=0.7
        )
        return response['choices'][0]['message']['content']
    except openai.error.OpenAIError as e:
        print(f"OpenAI error: {e}")
        raise e  # Reraise exception for retry logic
    except Exception as e:
        print(f"Unexpected error in GPT API call: {e}")
        raise e

def visualize_correlation(data, output_dir):
    """Generate and save a correlation heatmap."""
    # Select only numeric columns for correlation calculation
    numeric_data = data.select_dtypes(include=np.number)
    correlation_matrix = numeric_data.corr()
    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Correlation Heatmap")
    heatmap_path = os.path.join(output_dir, "correlation_heatmap.png")
    plt.savefig(heatmap_path, dpi=100)
    plt.close()
    return heatmap_path

def visualize_distributions(data, output_dir):
    """Generate and save feature distribution histograms."""
    numeric_data = data.select_dtypes(include=[np.number])
    num_columns = len(numeric_data.columns)
    num_rows = (num_columns // 3) + (1 if num_columns % 3 != 0 else 0)
    plt.figure(figsize=(12, 4 * num_rows))
    numeric_data.hist(bins=30, figsize=(12, 8))
    plt.tight_layout()
    histogram_path = os.path.join(output_dir, "feature_distributions.png")
    plt.savefig(histogram_path, dpi=100)
    plt.close()
    return histogram_path

def analyze_dataset(input_file):
    """Perform analysis and generate outputs."""
    # Load dataset
    try:
        # Try reading with 'latin-1' encoding
        data = pd.read_csv(input_file, encoding='latin-1')  # Change encoding here
        print(f"Dataset '{input_file}' loaded successfully!")
    except FileNotFoundError:
        print(f"Error: File '{input_file}' not found.")
        sys.exit(1)
    except UnicodeDecodeError:
        print(f"Error: File '{input_file}' has an unknown encoding. Try 'utf-8', 'latin-1', or 'iso-8859-1'.")
        sys.exit(1)

    # Create output directory
    output_dir = os.path.splitext(input_file)[0]
    os.makedirs(output_dir, exist_ok=True)

    # Dataset inspection
    missing_values = data.isnull().sum()
    total_missing = missing_values.sum()
    summary_stats = data.describe()

    # Detect outliers
    numeric_data = data.select_dtypes(include=[np.number]) # Select only numeric columns
    Q1 = numeric_data.quantile(0.25)
    Q3 = numeric_data.quantile(0.75)
    IQR = Q3 - Q1
    outliers = ((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).sum()

    # Generate visualizations
    heatmap_path = visualize_correlation(data, output_dir)
    histogram_path = visualize_distributions(data, output_dir)

    # GPT narrative
    prompt = f"""
    You are a data storytelling assistant. Please narrate the analysis of the dataset in a story format. The story should include:
    1. A detailed introduction to the dataset `{input_file}`, describing its structure, size, and potential use cases.
    2. The comprehensive analysis performed, such as handling missing values, detecting outliers, visualizing correlations, and identifying patterns.
    3. The key insights discovered, such as the total missing values ({total_missing}) and outliers ({outliers.sum()} across {len(outliers)} features), and their significance.
    4. The broader implications of these findings and specific recommendations for future steps, including potential strategies for data cleaning, preprocessing, and utilization.
    5. Incorporate vivid references to the visualizations generated: a correlation heatmap and feature distribution histograms, highlighting their role in the analysis.
    Be creative and elaborate on the story, making it engaging and informative for the reader.
    """
    try:
        narrative = call_gpt(prompt)
    except Exception as e:
        # Fallback narrative
        narrative = (
            f"Once upon a dataset, the file `{input_file}` revealed its secrets to the analyst. "
            f"It had {total_missing} missing values, some of which were crucial for understanding the bigger picture. "
            f"Outliers totaling {outliers.sum()} across {len(outliers)} features demanded attention, as they told tales of extremes and anomalies. "
            "Through the lens of a correlation heatmap and the colorful spread of histograms, patterns and relationships emerged. "
            "These insights urged the data scientists to clean, preprocess, and explore further, paving the way for impactful decisions."
        )

    # Save README
    readme_path = os.path.join(output_dir, "README.md")
    with open(readme_path, "w") as f:
        f.write("# Data Analysis Report\n")
        f.write(f"## Dataset: {input_file}\n\n")
        f.write("### Narrative\n")
        f.write(narrative + "\n\n")
        f.write("### Visualizations\n")
        f.write(f"![Correlation Heatmap]({heatmap_path})\n")
        f.write(f"![Feature Distributions]({histogram_path})\n")

    print(f"Analysis complete. Outputs saved in '{output_dir}'")

# Compatibility for Colab and Command-Line
if __name__ == "__main__":
    # Check if running in Colab
    if "google.colab" in sys.modules:
        from google.colab import files
        uploaded = files.upload()  # Upload your CSV file
        input_file = list(uploaded.keys())[0]  # Get uploaded filename
    elif len(sys.argv) < 2:
        print("Usage: python autolysis.py <dataset.csv>")
        sys.exit(1)
    else:
        input_file = sys.argv[1]  # Get filename from command-line arguments

    analyze_dataset(input_file)